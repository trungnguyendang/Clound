---
# =============================================================================
# P0: Config SSH no password (SSH TRUST)
# =============================================================================
- hosts: spark_master
  become: yes
  become_user: ubuntu
  tasks:
    - name: Generate SSH Keypair for Master
      user:
        name: ubuntu
        generate_ssh_key: yes
        ssh_key_bits: 2048
        ssh_key_file: .ssh/id_rsa

    - name: Configure SSH to disable Host Key Checking
      copy:
        dest: "/home/ubuntu/.ssh/config"
        content: |
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null
        owner: ubuntu
        group: ubuntu
        mode: '0600'

    - name: Fetch Master's Public Key to Local Machine
      fetch:
        src: /home/ubuntu/.ssh/id_rsa.pub
        dest: /tmp/master_id_rsa.pub
        flat: yes

- hosts: all
  become: yes
  tasks:
    - name: Add Master's Public Key to Authorized Keys
      authorized_key:
        user: ubuntu
        state: present
        key: "{{ lookup('file', '/tmp/master_id_rsa.pub') }}"

# =============================================================================
# P1: Installation and Configuration for All Nodes
# =============================================================================
- hosts: all
  become: yes
  vars:
    HADOOP_VERSION: "2.7.1"
    SPARK_VERSION: "2.4.3"
    SPARK_HADOOP_VERSION: "2.7"
    JAVA_HOME_PATH: "/usr/lib/jvm/java-8-openjdk-amd64"
    INSTALL_DIR: "/opt"
    hadoop_conf_dir: "/opt/hadoop-{{ HADOOP_VERSION }}/etc/hadoop"

  tasks:
    # --- CLEAN UP & PREPARE ---
    - name: Clean up old versions
      file:
        path: "{{ item }}"
        state: absent
      loop:
        - "/opt/hadoop-2.7.7"
        - "/opt/spark-2.4.3-bin-hadoop2.7"
        - "/opt/hadoop-{{ HADOOP_VERSION }}"
        - "/tmp/hadoop-ubuntu"

    - name: Map IP Master vào /etc/hosts
      lineinfile:
        path: /etc/hosts
        line: "10.0.1.10 spark-master"
        state: present

    # --- INSTALLATION ---
    - name: Update APT and install Java 8
      apt:
        name:
          - openjdk-8-jdk
          - wget
          - python3
        state: present
        update_cache: yes

    - name: Download Hadoop
      get_url:
        url: "https://archive.apache.org/dist/hadoop/common/hadoop-{{ HADOOP_VERSION }}/hadoop-{{ HADOOP_VERSION }}.tar.gz"
        dest: "/tmp/hadoop-{{ HADOOP_VERSION }}.tar.gz"

    - name: Extract Hadoop
      unarchive:
        src: "/tmp/hadoop-{{ HADOOP_VERSION }}.tar.gz"
        dest: "{{ INSTALL_DIR }}"
        remote_src: yes
        creates: "{{ INSTALL_DIR }}/hadoop-{{ HADOOP_VERSION }}"

    - name: Download Spark
      get_url:
        url: "https://archive.apache.org/dist/spark/spark-{{ SPARK_VERSION }}/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}.tgz"
        dest: "/tmp/spark-{{ SPARK_VERSION }}.tgz"

    - name: Extract Spark
      unarchive:
        src: "/tmp/spark-{{ SPARK_VERSION }}.tgz"
        dest: "{{ INSTALL_DIR }}"
        remote_src: yes
        creates: "{{ INSTALL_DIR }}/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}"

    # Phân quyền cho thư mục cài đặt
    - name: Change ownership to ubuntu
      file:
        path: "{{ item }}"
        owner: ubuntu
        group: ubuntu
        recurse: yes
      loop:
        - "{{ INSTALL_DIR }}/hadoop-{{ HADOOP_VERSION }}"
        - "{{ INSTALL_DIR }}/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}"

    # --- CONFIGURATION  ---

    - name: Setup environment variables
      template:
        src: "templates/profile.sh.j2"
        dest: "/etc/profile.d/spark_hadoop.sh"
        mode: 0755

    - name: Config hadoop-env.sh
      blockinfile:
        path: "{{ hadoop_conf_dir }}/hadoop-env.sh"
        block: |
          export JAVA_HOME={{ JAVA_HOME_PATH }}
          export HADOOP_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
        create: yes

    - name: Config core-site.xml
      template:
        src: "templates/core-site.xml.j2"
        dest: "{{ hadoop_conf_dir }}/core-site.xml"

    - name: Config hdfs-site.xml
      template:
        src: "templates/hdfs-site.xml.j2"
        dest: "{{ hadoop_conf_dir }}/hdfs-site.xml"

# =============================================================================
# P2: Configuration for Master Node
# =============================================================================
- hosts: spark_master
  become: yes
  vars:
    HADOOP_VERSION: "2.7.1"
    SPARK_VERSION: "2.4.3"
    SPARK_HADOOP_VERSION: "2.7"
    hadoop_conf_dir: "/opt/hadoop-{{ HADOOP_VERSION }}/etc/hadoop"
    spark_conf_dir: "/opt/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}/conf"

  tasks:
    - name: Config Hadoop slaves
      copy:
        content: |
          spark-worker-1
          spark-worker-2
        dest: "{{ hadoop_conf_dir }}/slaves"

    - name: Config Spark slaves
      copy:
        content: |
          spark-worker-1
          spark-worker-2
        dest: "{{ spark_conf_dir }}/slaves"

    # [SỬA] Chỉ giữ lại 1 task config này (có cả Host và SSH Opts)
    - name: Config spark-env.sh (Master Host & SSH Opts)
      blockinfile:
        path: "{{ spark_conf_dir }}/spark-env.sh"
        block: |
          export SPARK_MASTER_HOST=spark-master
          export SPARK_SSH_OPTS="-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null"
        create: yes

    # [QUAN TRỌNG] Task này đảm bảo quyền sở hữu file log thuộc về ubuntu
    - name: Fix permissions recursively for Hadoop and Spark
      file:
        path: "{{ item }}"
        owner: ubuntu
        group: ubuntu
        recurse: yes
      loop:
        - "/opt/hadoop-{{ HADOOP_VERSION }}"
        - "/opt/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}"

    # 1. Format NameNode (Chạy bằng user ubuntu)
    - name: Format HDFS NameNode (Only if not formatted)
      shell: |
        . /etc/profile.d/spark_hadoop.sh
        hdfs namenode -format -force
      args:
        executable: /bin/bash
        creates: /tmp/hadoop-ubuntu/dfs/name/current/VERSION
      become: yes
      become_user: ubuntu # <--- KEY FIX PERMISSION ERROR

    # 2. Khởi động HDFS (Chạy bằng user ubuntu)
    - name: Start HDFS Cluster
      shell: |
        . /etc/profile.d/spark_hadoop.sh
        stop-dfs.sh
        start-dfs.sh
      args:
        executable: /bin/bash
      register: hdfs_start_output
      become: yes
      become_user: ubuntu # <--- KEY FIX PERMISSION ERROR

    - name: Print HDFS Start Log
      debug:
        var: hdfs_start_output.stdout_lines

    # Đợi NameNode thực sự lên (chỉ check cổng, bỏ check host)
    - name: Wait for NameNode to start listening on port 9000
      wait_for:
#        host: "spark-master"  # Kiểm tra đúng host spark-master (đã map trong /etc/hosts)
#        port: 9000
#        delay: 10
#        timeout: 60
#        state: started
        path: /opt/hadoop-2.7.1/logs/hadoop-ubuntu-namenode-spark-master.log
        search_regex: "NameNode RPC up at"
        timeout: 60

    # 3. Tạo thư mục Input trên HDFS (Chạy bằng user ubuntu)
    - name: Create HDFS Input Directory (/input)
      shell: |
        . /etc/profile.d/spark_hadoop.sh
        hdfs dfs -mkdir -p /input
      args:
        executable: /bin/bash
      become: yes
      become_user: ubuntu # <--- KEY FIX PERMISSION ERROR

    # 4. Khởi động Spark (Chạy bằng user ubuntu)
    - name: Start Spark Cluster
      shell: |
        . /etc/profile.d/spark_hadoop.sh
        stop-all.sh || true
        start-master.sh
        start-slaves.sh
      args:
        executable: /bin/bash
      register: spark_start_output
      become: yes
      become_user: ubuntu # <--- KEY FIX PERMISSION ERROR

    - name: Print Spark Start Log
      debug:
        var: spark_start_output.stdout_lines


# =============================================================================
# P3: Configuration for Worker Nodes
# =============================================================================
- hosts: spark_workers
  become: yes
  vars:
    SPARK_VERSION: "2.4.3"
    SPARK_HADOOP_VERSION: "2.7"
    spark_conf_dir: "/opt/spark-{{ SPARK_VERSION }}-bin-hadoop{{ SPARK_HADOOP_VERSION }}/conf"

  tasks:
    - name: Config spark-env.sh on Workers
      lineinfile:
        path: "{{ spark_conf_dir }}/spark-env.sh"
        line: "export SPARK_MASTER_HOST=spark-master"
        create: yes